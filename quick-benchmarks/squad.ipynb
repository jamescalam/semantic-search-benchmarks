{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets torch transformers sentence-transformers hnswlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQuAD Benchmark HNSW\n",
    "\n",
    "In this notebook we will work through an **A**pproximate **N**earest **N**eighbors **S**earch (ANNS) benchmark using modern embedding models and datasets. Here we will use the **S**tanford **Qu**estion and **A**nswering **D**ataset (SQuAD) and a MPNet sentence transformer model trained for question-answering.\n",
    "\n",
    "## Building Embeddings\n",
    "\n",
    "We start by initializing the dataset and creating both the query and context embeddings that we will be searching with. The dataset is hosted on Hugging Face *Datasets*, and we initialize like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading builder script: 5.27kB [00:00, 3.16MB/s]                   \n",
      "Downloading metadata: 2.36kB [00:00, 1.87MB/s]                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset squad/plain_text (download: 33.51 MiB, generated: 85.63 MiB, post-processed: Unknown size, total: 119.14 MiB) to /home/jupyter/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|          | 0.00/8.12M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  91%|█████████▏| 7.42M/8.12M [00:00<00:00, 74.2MB/s]\u001b[A\n",
      "Downloading data: 16.4MB [00:00, 83.2MB/s]                            \u001b[A\n",
      "Downloading data: 30.3MB [00:00, 81.8MB/s]\u001b[A\n",
      "Downloading data files:  50%|█████     | 1/2 [00:01<00:01,  1.25s/it]\n",
      "Downloading data: 4.85MB [00:00, 72.5MB/s]                   \u001b[A\n",
      "Downloading data files: 100%|██████████| 2/2 [00:01<00:00,  1.26it/s]\n",
      "Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 1312.36it/s]\n",
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset squad downloaded and prepared to /home/jupyter/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 87599\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "squad = load_dataset('squad', split='train')\n",
    "squad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the encodings, we initialize an embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = SentenceTransformer('multi-qa-mpnet-base-dot-v1', device=device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be encoding all unique contexts from SQuAD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18891"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts = list(set(squad['context']))\n",
    "len(contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After encoding, we will return embeddings of dimensionality `768`. The embedding dimensionality is specific to each embedding model, and we can check that this is correct via the `model.get_sentence_embedding_dimension` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dims = model.get_sentence_embedding_dimension()\n",
    "dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we encode all of the contexts. We do this in batches to avoid overloading the limited RAM of our machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 296/296 [01:15<00:00,  3.94it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(18891, 768)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "batch_size = 64\n",
    "# initialize zero array where we later add all context embeddings\n",
    "encodings = np.zeros((len(contexts), dims))\n",
    "\n",
    "for i in tqdm(range(0, len(contexts), batch_size)):\n",
    "    # find batch size\n",
    "    i_end = min(i + batch_size, len(contexts))\n",
    "    # create encodings\n",
    "    embeddings = model.encode(contexts[i:i_end])\n",
    "    # add to encodings array\n",
    "    encodings[i:i_end] = embeddings\n",
    "\n",
    "# normalize\n",
    "encodings = encodings / np.linalg.norm(encodings, axis=1, keepdims=True)\n",
    "encodings.shape\n",
    "# save to file\n",
    "with open('squad.npy', 'wb') as f:\n",
    "    np.save(f, encodings)\n",
    "\n",
    "encodings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must do the same with our questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = list(set(squad['question']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1365/1365 [00:39<00:00, 34.46it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(87355, 768)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# again, we initialize the zero array where we will add query embeddings\n",
    "xq_arr = np.zeros((len(questions), dims))\n",
    "\n",
    "for i in tqdm(range(0, len(questions), batch_size)):\n",
    "    # find batch size\n",
    "    i_end = min(i + batch_size, len(questions))\n",
    "    # create encodings\n",
    "    embeddings = model.encode(questions[i:i_end])\n",
    "    # add to encodings array\n",
    "    xq_arr[i:i_end] = embeddings\n",
    "\n",
    "# save to file\n",
    "with open('squad_xq.npy', 'wb') as f:\n",
    "    np.save(f, xq_arr)\n",
    "\n",
    "xq_arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can begin testing. First set BLAS libraries to use a single thread (eg making Numpy matmul op use a single thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# setting params so BLAS libraries (numpy matmul) is only using a single thread\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\" # export OMP_NUM_THREADS=4\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\" # export OPENBLAS_NUM_THREADS=4 \n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\" # export MKL_NUM_THREADS=6\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\" # export VECLIB_MAXIMUM_THREADS=4\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\" # export NUMEXPR_NUM_THREADS=6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do this to be comparable to `hnswlib` that we use later. Now perform the full kNN search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = np.matmul(xq_arr, encodings.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now calculate the k-NN baseline for @1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = np.argmax(dist, axis=1).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything else is calculated from this, so we now move on to performing the same operations but with HNSW.\n",
    "\n",
    "We first initialize a HNSW index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hnswlib\n",
    "\n",
    "index = hnswlib.Index(space='ip', dim=dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then build the index using the contexts we have (number of elements should be known before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.init_index(\n",
    "    max_elements=encodings.shape[0],\n",
    "    ef_construction=1000,\n",
    "    M=24\n",
    ")\n",
    "index.add_items(encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different parameters produce different performance for the HNSW index, we need to test with varying parameters to find which works best with the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [39:47<00:00, 119.36s/it]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "hnsw_perf = pd.DataFrame({\n",
    "    'ef': [],\n",
    "    'qps': [],\n",
    "    'recall@1': []\n",
    "})\n",
    "\n",
    "index.set_num_threads(1)\n",
    "ef_vals = [10,20,50,100,110,120,130,135,140,145,150,200,300,400,500,600,700,800,900,1000]\n",
    "\n",
    "# we will test HNSW with many different ef search values\n",
    "for ef in tqdm(ef_vals):\n",
    "    index.set_ef(ef) # ef should always be > k\n",
    "    # Query dataset, k - number of closest elements (returns 2 numpy arrays)\n",
    "    t0=time.time()\n",
    "    labels, distances = index.knn_query(xq_arr, k = 1)\n",
    "    # calculate queries per second (QPS)\n",
    "    qps=len(xq_arr)/(time.time()-t0)\n",
    "    # calculate recall@k\n",
    "    recall = np.sum(\n",
    "        labels.reshape(-1) == baseline.reshape(-1)\n",
    "    ) / len(xq_arr)\n",
    "    hnsw_perf = hnsw_perf.append({\n",
    "        'ef': ef, 'qps': qps, 'recall@1': recall\n",
    "    }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ef</th>\n",
       "      <th>qps</th>\n",
       "      <th>recall@1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.0</td>\n",
       "      <td>9970.085016</td>\n",
       "      <td>0.947318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.0</td>\n",
       "      <td>6183.901004</td>\n",
       "      <td>0.979028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50.0</td>\n",
       "      <td>3039.892434</td>\n",
       "      <td>0.995158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.0</td>\n",
       "      <td>1722.959084</td>\n",
       "      <td>0.998890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>110.0</td>\n",
       "      <td>1589.131769</td>\n",
       "      <td>0.999061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>120.0</td>\n",
       "      <td>1479.290717</td>\n",
       "      <td>0.999222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>130.0</td>\n",
       "      <td>1382.523561</td>\n",
       "      <td>0.999325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>135.0</td>\n",
       "      <td>1343.248343</td>\n",
       "      <td>0.999416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>140.0</td>\n",
       "      <td>1305.419935</td>\n",
       "      <td>0.999508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>145.0</td>\n",
       "      <td>1274.720829</td>\n",
       "      <td>0.999531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>150.0</td>\n",
       "      <td>1230.076477</td>\n",
       "      <td>0.999576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>200.0</td>\n",
       "      <td>965.674629</td>\n",
       "      <td>0.999771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>300.0</td>\n",
       "      <td>703.989417</td>\n",
       "      <td>0.999943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>400.0</td>\n",
       "      <td>562.880942</td>\n",
       "      <td>0.999989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>500.0</td>\n",
       "      <td>475.164416</td>\n",
       "      <td>0.999989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>600.0</td>\n",
       "      <td>415.557137</td>\n",
       "      <td>0.999989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>700.0</td>\n",
       "      <td>372.202735</td>\n",
       "      <td>0.999989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>800.0</td>\n",
       "      <td>338.620944</td>\n",
       "      <td>0.999989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>900.0</td>\n",
       "      <td>312.854962</td>\n",
       "      <td>0.999989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>290.792129</td>\n",
       "      <td>0.999989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ef          qps  recall@1\n",
       "0     10.0  9970.085016  0.947318\n",
       "1     20.0  6183.901004  0.979028\n",
       "2     50.0  3039.892434  0.995158\n",
       "3    100.0  1722.959084  0.998890\n",
       "4    110.0  1589.131769  0.999061\n",
       "5    120.0  1479.290717  0.999222\n",
       "6    130.0  1382.523561  0.999325\n",
       "7    135.0  1343.248343  0.999416\n",
       "8    140.0  1305.419935  0.999508\n",
       "9    145.0  1274.720829  0.999531\n",
       "10   150.0  1230.076477  0.999576\n",
       "11   200.0   965.674629  0.999771\n",
       "12   300.0   703.989417  0.999943\n",
       "13   400.0   562.880942  0.999989\n",
       "14   500.0   475.164416  0.999989\n",
       "15   600.0   415.557137  0.999989\n",
       "16   700.0   372.202735  0.999989\n",
       "17   800.0   338.620944  0.999989\n",
       "18   900.0   312.854962  0.999989\n",
       "19  1000.0   290.792129  0.999989"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hnsw_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m95"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b8e7999f96e1b425e2d542f21b571f5a4be3e97158b0b46ea1b2500df63956ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
